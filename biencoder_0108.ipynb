{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adb16359-b371-415e-b03e-3c8e37734f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python is /home/wyeh/anaconda3/envs/biocreative/bin/python\n"
     ]
    }
   ],
   "source": [
    "!type python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7875375b-11fe-4a4e-97d2-9239481dba06",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Copyright (c) Facebook, Inc. and its affiliates.\n",
    "All rights reserved.\n",
    "\n",
    "This source code is licensed under the license found in the\n",
    "LICENSE file in the root directory of this source tree.\n",
    "'''\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from nltk.corpus import wordnet as wn\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from transformers import *\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from wsd_models.util import *\n",
    "from wsd_models.models import BiEncoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b87ee85-eec1-4136-ada4-f418d8771b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--split'], dest='split', nargs=None, const=None, default='semeval2007', type=<class 'str'>, choices=['semeval2007', 'senseval2', 'senseval3', 'semeval2013', 'semeval2015', 'ALL', 'all-test'], help='Which evaluation split on which to evaluate probe', metavar=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Gloss Informed Bi-encoder for WSD')\n",
    "\n",
    "#training arguments\n",
    "parser.add_argument('--rand_seed', type=int, default=42)\n",
    "parser.add_argument('--grad-norm', type=float, default=1.0)\n",
    "parser.add_argument('--silent', action='store_true',\n",
    "\thelp='Flag to supress training progress bar for each epoch')\n",
    "parser.add_argument('--multigpu', action='store_true')\n",
    "parser.add_argument('--lr', type=float, default=0.00001)\n",
    "parser.add_argument('--warmup', type=int, default=10000)\n",
    "parser.add_argument('--context-max-length', type=int, default=128)\n",
    "parser.add_argument('--gloss-max-length', type=int, default=32)\n",
    "parser.add_argument('--epochs', type=int, default=20)\n",
    "parser.add_argument('--context-bsz', type=int, default=4)\n",
    "parser.add_argument('--gloss-bsz', type=int, default=256)\n",
    "parser.add_argument('--encoder-name', type=str, default='bert-base',\n",
    "\tchoices=['bert-base', 'bert-large', 'roberta-base', 'roberta-large'])\n",
    "parser.add_argument('--ckpt', type=str, default='model_0108',\n",
    "\thelp='filepath at which to save best probing model (on dev set)')\n",
    "parser.add_argument('--data-path', type=str, default='WSD_Evaluation_Framework',\n",
    "\thelp='Location of top-level directory for the Unified WSD Framework')\n",
    "\n",
    "#sets which parts of the model to freeze ❄️ during training for ablation \n",
    "parser.add_argument('--freeze-context', action='store_true')\n",
    "parser.add_argument('--freeze-gloss', action='store_true')\n",
    "parser.add_argument('--tie-encoders', action='store_true')\n",
    "\n",
    "#other training settings flags\n",
    "parser.add_argument('--kshot', type=int, default=-1,\n",
    "\thelp='if set to k (1+), will filter training data to only have up to k examples per sense')\n",
    "parser.add_argument('--balanced', action='store_true',\n",
    "\thelp='flag for whether or not to reweight sense losses to be balanced wrt the target word')\n",
    "\n",
    "#evaluation arguments\n",
    "parser.add_argument('--eval', action='store_true',\n",
    "\thelp='Flag to set script to evaluate probe (rather than train)')\n",
    "parser.add_argument('--split', type=str, default='semeval2007',\n",
    "\tchoices=['semeval2007', 'senseval2', 'senseval3', 'semeval2013', 'semeval2015', 'ALL', 'all-test'],\n",
    "\thelp='Which evaluation split on which to evaluate probe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bccffb9-8b2f-41bd-91ae-2811b1f89544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--rand_seed RAND_SEED]\n",
      "                             [--grad-norm GRAD_NORM] [--silent] [--multigpu]\n",
      "                             [--lr LR] [--warmup WARMUP]\n",
      "                             [--context-max-length CONTEXT_MAX_LENGTH]\n",
      "                             [--gloss-max-length GLOSS_MAX_LENGTH]\n",
      "                             [--epochs EPOCHS] [--context-bsz CONTEXT_BSZ]\n",
      "                             [--gloss-bsz GLOSS_BSZ]\n",
      "                             [--encoder-name {bert-base,bert-large,roberta-base,roberta-large}]\n",
      "                             [--ckpt CKPT] [--data-path DATA_PATH]\n",
      "                             [--freeze-context] [--freeze-gloss]\n",
      "                             [--tie-encoders] [--kshot KSHOT] [--balanced]\n",
      "                             [--eval]\n",
      "                             [--split {semeval2007,senseval2,senseval3,semeval2013,semeval2015,ALL,all-test}]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/wyeh/.local/share/jupyter/runtime/kernel-b87fc30f-24fe-4f42-9f3b-8292f05ad666.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "#uses these two gpus if training in multi-gpu\n",
    "context_device = \"cuda:0\"\n",
    "gloss_device = \"cuda:1\"\n",
    "\n",
    "def tokenize_glosses(gloss_arr, tokenizer, max_len):\n",
    "\tglosses = []\n",
    "\tmasks = []\n",
    "\tfor gloss_text in gloss_arr:\n",
    "\t\tg_ids = [torch.tensor([[x]]) \n",
    "                 for x in tokenizer.encode(tokenizer.cls_token)+tokenizer.encode(gloss_text)+tokenizer.encode(tokenizer.sep_token)]\n",
    "\t\tg_attn_mask = [1]*len(g_ids)\n",
    "\t\tg_fake_mask = [-1]*len(g_ids)\n",
    "\t\tg_ids, g_attn_mask, _ = normalize_length(g_ids, g_attn_mask, g_fake_mask, max_len, pad_id=tokenizer.encode(tokenizer.pad_token)[0])\n",
    "\t\tg_ids = torch.cat(g_ids, dim=-1)\n",
    "\t\tg_attn_mask = torch.tensor(g_attn_mask)\n",
    "\t\tglosses.append(g_ids)\n",
    "\t\tmasks.append(g_attn_mask)\n",
    "\n",
    "\treturn glosses, masks\n",
    "\n",
    "#creates a sense label/ gloss dictionary for training/using the gloss encoder\n",
    "def load_and_preprocess_glosses(data, tokenizer, wn_senses, max_len=-1):\n",
    "\tsense_glosses = {}\n",
    "\tsense_weights = {}\n",
    "\n",
    "\tgloss_lengths = []\n",
    "\n",
    "\tfor sent in data:\n",
    "\t\tfor _, lemma, pos, _, label in sent:\n",
    "\t\t\tif label == -1:\n",
    "\t\t\t\tcontinue #ignore unlabeled words\n",
    "\t\t\telse:\n",
    "\t\t\t\tkey = generate_key(lemma, pos)\n",
    "\t\t\t\tif key not in sense_glosses:\n",
    "\t\t\t\t\t#get all sensekeys for the lemma/pos pair\n",
    "\t\t\t\t\tsensekey_arr = wn_senses[key]\n",
    "\t\t\t\t\t#get glosses for all candidate senses\n",
    "\t\t\t\t\tgloss_arr = [wn.lemma_from_key(s).synset().definition() for s in sensekey_arr]\n",
    "\n",
    "\t\t\t\t\t#preprocess glosses into tensors\n",
    "\t\t\t\t\tgloss_ids, gloss_masks = tokenize_glosses(gloss_arr, tokenizer, max_len)\n",
    "\t\t\t\t\tgloss_ids = torch.cat(gloss_ids, dim=0)\n",
    "\t\t\t\t\tgloss_masks = torch.stack(gloss_masks, dim=0)\n",
    "\t\t\t\t\tsense_glosses[key] = (gloss_ids, gloss_masks, sensekey_arr)\n",
    "\n",
    "\t\t\t\t\t#intialize weights for balancing senses\n",
    "\t\t\t\t\tsense_weights[key] = [0]*len(gloss_arr)\n",
    "\t\t\t\t\tw_idx = sensekey_arr.index(label)\n",
    "\t\t\t\t\tsense_weights[key][w_idx] += 1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t#update sense weight counts\n",
    "\t\t\t\t\tw_idx = sense_glosses[key][2].index(label)\n",
    "\t\t\t\t\tsense_weights[key][w_idx] += 1\n",
    "\t\t\t\t\n",
    "\t\t\t\t#make sure that gold label is retrieved synset\n",
    "\t\t\t\tassert label in sense_glosses[key][2]\n",
    "\n",
    "\t#normalize weights\n",
    "\tfor key in sense_weights:\n",
    "\t\ttotal_w = sum(sense_weights[key])\n",
    "\t\tsense_weights[key] = torch.FloatTensor([total_w/x if x !=0 else 0 for x in sense_weights[key]])\n",
    "\n",
    "\treturn sense_glosses, sense_weights\n",
    "\n",
    "def preprocess_context(tokenizer, text_data, bsz=1, max_len=-1):\n",
    "\tif max_len == -1: assert bsz==1 #otherwise need max_length for padding\n",
    "\n",
    "\tcontext_ids = []\n",
    "\tcontext_attn_masks = []\n",
    "\n",
    "\texample_keys = []\n",
    "\n",
    "\tcontext_output_masks = []\n",
    "\tinstances = []\n",
    "\tlabels = []\n",
    "\n",
    "\t#tensorize data\n",
    "\tfor sent in text_data:\n",
    "\t\tc_ids = [torch.tensor([tokenizer.encode(tokenizer.cls_token)])] #cls token aka sos token, returns a list with index\n",
    "\t\to_masks = [-1]\n",
    "\t\tsent_insts = []\n",
    "\t\tsent_keys = []\n",
    "\t\tsent_labels = []\n",
    "\n",
    "\t\t#For each word in sentence...\n",
    "\t\tfor idx, (word, lemma, pos, inst, label) in enumerate(sent):\n",
    "\t\t\t#tensorize word for context ids\n",
    "\t\t\tword_ids = [torch.tensor([[x]]) for x in tokenizer.encode(word.lower())]\n",
    "\t\t\tc_ids.extend(word_ids)\n",
    "\n",
    "\t\t\t#if word is labeled with WSD sense...\n",
    "\t\t\tif inst != -1:\n",
    "\t\t\t\t#add word to bert output mask to be labeled\n",
    "\t\t\t\to_masks.extend([idx]*len(word_ids))\n",
    "\t\t\t\t#track example instance id\n",
    "\t\t\t\tsent_insts.append(inst)\n",
    "\t\t\t\t#track example instance keys to get glosses\n",
    "\t\t\t\tex_key = generate_key(lemma, pos)\n",
    "\t\t\t\tsent_keys.append(ex_key)\n",
    "\t\t\t\tsent_labels.append(label)\n",
    "\t\t\telse:\n",
    "\t\t\t\t#mask out output of context encoder for WSD task (not labeled)\n",
    "\t\t\t\to_masks.extend([-1]*len(word_ids))\n",
    "\n",
    "\t\t\t#break if we reach max len\n",
    "\t\t\tif max_len != -1 and len(c_ids) >= (max_len-1):\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\tc_ids.append(torch.tensor([tokenizer.encode(tokenizer.sep_token)])) #aka eos token\n",
    "\t\tc_attn_mask = [1]*len(c_ids)\n",
    "\t\to_masks.append(-1)\n",
    "\t\tc_ids, c_attn_masks, o_masks = normalize_length(c_ids, c_attn_mask, o_masks, max_len, pad_id=tokenizer.encode(tokenizer.pad_token)[0])\n",
    "\n",
    "\t\ty = torch.tensor([1]*len(sent_insts), dtype=torch.float)\n",
    "\t\t#not including examples sentences with no annotated sense data\n",
    "\t\tif len(sent_insts) > 0:\n",
    "\t\t\tcontext_ids.append(torch.cat(c_ids, dim=-1))\n",
    "\t\t\tcontext_attn_masks.append(torch.tensor(c_attn_masks).unsqueeze(dim=0))\n",
    "\t\t\tcontext_output_masks.append(torch.tensor(o_masks).unsqueeze(dim=0))\n",
    "\t\t\texample_keys.append(sent_keys)\n",
    "\t\t\tinstances.append(sent_insts)\n",
    "\t\t\tlabels.append(sent_labels)\n",
    "\n",
    "\t#package data\n",
    "\tdata = list(zip(context_ids, context_attn_masks, context_output_masks, example_keys, instances, labels))\n",
    "\n",
    "\t#batch data if bsz > 1\n",
    "\tif bsz > 1:\n",
    "\t\tprint('Batching data with bsz={}...'.format(bsz))\n",
    "\t\tbatched_data = []\n",
    "\t\tfor idx in range(0, len(data), bsz):\n",
    "\t\t\tif idx+bsz <=len(data): b = data[idx:idx+bsz]\n",
    "\t\t\telse: b = data[idx:]\n",
    "\t\t\tcontext_ids = torch.cat([x for x,_,_,_,_,_ in b], dim=0)\n",
    "\t\t\tcontext_attn_mask = torch.cat([x for _,x,_,_,_,_ in b], dim=0)\n",
    "\t\t\tcontext_output_mask = torch.cat([x for _,_,x,_,_,_ in b], dim=0)\n",
    "\t\t\texample_keys = []\n",
    "\t\t\tfor _,_,_,x,_,_ in b: example_keys.extend(x)\n",
    "\t\t\tinstances = []\n",
    "\t\t\tfor _,_,_,_,x,_ in b: instances.extend(x)\n",
    "\t\t\tlabels = []\n",
    "\t\t\tfor _,_,_,_,_,x in b: labels.extend(x)\n",
    "\t\t\tbatched_data.append((context_ids, context_attn_mask, context_output_mask, example_keys, instances, labels))\n",
    "\t\treturn batched_data\n",
    "\telse:  \n",
    "\t\treturn data\n",
    "\n",
    "def _train(train_data, model, gloss_dict, optim, schedule, criterion, gloss_bsz=-1, \n",
    "           max_grad_norm=1.0, multigpu=False, silent=False, train_steps=-1):\n",
    "\tmodel.train()\n",
    "\ttotal_loss = 0.\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\n",
    "\ttrain_data = enumerate(train_data)\n",
    "\tif not silent: train_data = tqdm(list(train_data))\n",
    "\n",
    "\tfor i, (context_ids, context_attn_mask, context_output_mask, example_keys, _, labels) in train_data:\n",
    "\n",
    "\t\t#reset model\n",
    "\t\tmodel.zero_grad()\n",
    "\t\t#run example sentence(s) through context encoder\n",
    "\t\tif multigpu:\n",
    "\t\t\tcontext_ids = context_ids.to(context_device)\n",
    "\t\t\tcontext_attn_mask = context_attn_mask.to(context_device)\n",
    "\t\telse:\n",
    "\t\t\tcontext_ids = context_ids.cuda()\n",
    "\t\t\tcontext_attn_mask = context_attn_mask.cuda()\n",
    "\t\tcontext_output = model.context_forward(context_ids, context_attn_mask, context_output_mask)\n",
    "\n",
    "\t\tloss = 0.\n",
    "\t\tgloss_sz = 0\n",
    "\t\tcontext_sz = len(labels)\n",
    "\t\tfor j, (key, label) in enumerate(zip(example_keys, labels)):\n",
    "\t\t\toutput = context_output.split(1,dim=0)[j]\n",
    "\n",
    "\t\t\t#run example's glosses through gloss encoder\n",
    "\t\t\tgloss_ids, gloss_attn_mask, sense_keys = gloss_dict[key]\n",
    "\t\t\tif multigpu:\n",
    "\t\t\t\tgloss_ids = gloss_ids.to(gloss_device)\n",
    "\t\t\t\tgloss_attn_mask = gloss_attn_mask.to(gloss_device)\n",
    "\t\t\telse:\n",
    "\t\t\t\tgloss_ids = gloss_ids.cuda()\n",
    "\t\t\t\tgloss_attn_mask = gloss_attn_mask.cuda()\n",
    "\n",
    "\t\t\tgloss_output = model.gloss_forward(gloss_ids, gloss_attn_mask)\n",
    "\t\t\tgloss_output = gloss_output.transpose(0,1)\n",
    "\t\t\t\n",
    "\t\t\t#get cosine sim of example from context encoder with gloss embeddings\n",
    "\t\t\tif multigpu:\n",
    "\t\t\t\toutput = output.cpu()\n",
    "\t\t\t\tgloss_output = gloss_output.cpu()\n",
    "\t\t\t\n",
    "\t\t\toutput = torch.mm(output, gloss_output)\n",
    "\n",
    "\t\t\t#get label and calculate loss\n",
    "\t\t\tidx = sense_keys.index(label)\n",
    "\t\t\tlabel_tensor = torch.tensor([idx])\n",
    "\t\t\tif not multigpu: label_tensor = label_tensor.cuda()\n",
    "\n",
    "\t\t\t#looks up correct candidate senses criterion\n",
    "\t\t\t#needed if balancing classes within the candidate senses of a target word\n",
    "\t\t\tloss += criterion[key](output, label_tensor)\n",
    "\t\t\tgloss_sz += gloss_output.size(-1)\n",
    "\n",
    "\t\t\tif gloss_bsz != -1 and gloss_sz >= gloss_bsz:\n",
    "\t\t\t\t#update model\n",
    "\t\t\t\ttotal_loss += loss.item()\n",
    "\t\t\t\tloss=loss/gloss_sz\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\ttorch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\t\t\t\toptim.step()\n",
    "\t\t\t\tschedule.step() # Update learning rate schedule\n",
    "\n",
    "\t\t\t\t#reset loss and gloss_sz\n",
    "\t\t\t\tloss = 0.\n",
    "\t\t\t\tgloss_sz = 0\n",
    "\n",
    "\t\t\t\t#reset model\n",
    "\t\t\t\tmodel.zero_grad()\n",
    "\n",
    "\t\t\t\t#rerun context through model\n",
    "\t\t\t\tcontext_output = model.context_forward(context_ids, context_attn_mask, context_output_mask)\n",
    "\n",
    "\t\t#update model after finishing context batch\n",
    "\t\tif gloss_bsz != -1: loss_sz = gloss_sz\n",
    "\t\telse: loss_sz = context_sz\n",
    "\t\tif loss_sz > 0:\n",
    "\t\t\ttotal_loss += loss.item()\n",
    "\t\t\tloss=loss/loss_sz\n",
    "\t\t\tloss.backward()\n",
    "\t\t\ttorch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\t\t\toptim.step()\n",
    "\t\t\tschedule.step() # Update learning rate schedule\n",
    "\n",
    "\t\t#stop epoch early if number of training steps is reached\n",
    "\t\tif train_steps > 0 and i+1 == train_steps: break\n",
    "\n",
    "\treturn model, optim, schedule, total_loss\n",
    "\n",
    "def _eval(eval_data, model, gloss_dict, multigpu=False):\n",
    "\tmodel.eval()\n",
    "\teval_preds = []\n",
    "\tfor context_ids, context_attn_mask, context_output_mask, example_keys, insts, _ in eval_data:\n",
    "\t\twith torch.no_grad(): \n",
    "\t\t\t#run example through model\n",
    "\t\t\tif multigpu:\n",
    "\t\t\t\tcontext_ids = context_ids.to(context_device)\n",
    "\t\t\t\tcontext_attn_mask = context_attn_mask.to(context_device)\n",
    "\t\t\telse:\n",
    "\t\t\t\tcontext_ids = context_ids.cuda()\n",
    "\t\t\t\tcontext_attn_mask = context_attn_mask.cuda()\n",
    "\t\t\tcontext_output = model.context_forward(context_ids, context_attn_mask, context_output_mask)\n",
    "\n",
    "\t\t\tfor output, key, inst in zip(context_output.split(1,dim=0), example_keys, insts):\n",
    "\t\t\t\t#run example's glosses through gloss encoder\n",
    "\t\t\t\tgloss_ids, gloss_attn_mask, sense_keys = gloss_dict[key]\n",
    "\t\t\t\tif multigpu:\n",
    "\t\t\t\t\tgloss_ids = gloss_ids.to(gloss_device)\n",
    "\t\t\t\t\tgloss_attn_mask = gloss_attn_mask.to(gloss_device)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tgloss_ids = gloss_ids.cuda()\n",
    "\t\t\t\t\tgloss_attn_mask = gloss_attn_mask.cuda()\n",
    "\t\t\t\tgloss_output = model.gloss_forward(gloss_ids, gloss_attn_mask)\n",
    "\t\t\t\tgloss_output = gloss_output.transpose(0,1)\n",
    "\n",
    "\t\t\t\t#get cosine sim of example from context encoder with gloss embeddings\n",
    "\t\t\t\tif multigpu:\n",
    "\t\t\t\t\toutput = output.cpu()\n",
    "\t\t\t\t\tgloss_output = gloss_output.cpu()\n",
    "\t\t\t\toutput = torch.mm(output, gloss_output)\n",
    "\t\t\t\tpred_idx = output.topk(1, dim=-1)[1].squeeze().item()\n",
    "\t\t\t\tpred_label = sense_keys[pred_idx]\n",
    "\t\t\t\teval_preds.append((inst, pred_label))\n",
    "\n",
    "\treturn eval_preds\n",
    "\n",
    "def train_model(args):\n",
    "\tprint('Training WSD bi-encoder model...')\n",
    "\tif args.freeze_gloss: assert args.gloss_bsz == -1 #no gloss bsz if not training gloss encoder, memory concerns\n",
    "\n",
    "\t#create passed in ckpt dir if doesn't exist\n",
    "\tif not os.path.exists(args.ckpt): os.mkdir(args.ckpt)\n",
    "\n",
    "\t'''\n",
    "\tLOAD PRETRAINED TOKENIZER, TRAIN AND DEV DATA\n",
    "\t'''\n",
    "\tprint('Loading data + preprocessing...')\n",
    "\tsys.stdout.flush()\n",
    "\n",
    "\ttokenizer = load_tokenizer(args.encoder_name)\n",
    "\n",
    "\t#loading WSD (semcor) data\n",
    "\ttrain_path = os.path.join(args.data_path, 'Training_Corpora/SemCor/')\n",
    "\ttrain_data = load_data(train_path, 'semcor')\n",
    "\n",
    "\t#filter train data for k-shot learning\n",
    "\tif args.kshot > 0: train_data = filter_k_examples(train_data, args.kshot)\n",
    "\n",
    "\t#dev set = semeval2007\n",
    "\tsemeval2007_path = os.path.join(args.data_path, 'Evaluation_Datasets/semeval2007/')\n",
    "\tsemeval2007_data = load_data(semeval2007_path, 'semeval2007')\n",
    "\n",
    "\t#load gloss dictionary (all senses from wordnet for each lemma/pos pair that occur in data)\n",
    "\twn_path = os.path.join(args.data_path, 'Data_Validation/candidatesWN30.txt')\n",
    "\twn_senses = load_wn_senses(wn_path)\n",
    "\ttrain_gloss_dict, train_gloss_weights = load_and_preprocess_glosses(train_data, tokenizer, wn_senses, max_len=args.gloss_max_length)\n",
    "\tsemeval2007_gloss_dict, _ = load_and_preprocess_glosses(semeval2007_data, tokenizer, wn_senses, max_len=args.gloss_max_length)\n",
    "\n",
    "\t#preprocess and batch data (context + glosses)\n",
    "\ttrain_data = preprocess_context(tokenizer, train_data, bsz=args.context_bsz, max_len=args.context_max_length)\n",
    "\tsemeval2007_data = preprocess_context(tokenizer, semeval2007_data, bsz=1, max_len=args.context_max_length)\n",
    "\n",
    "\tepochs = args.epochs\n",
    "\toverflow_steps = -1\n",
    "\tt_total = len(train_data)*epochs\n",
    "\n",
    "\t#if few-shot training, override epochs to calculate num. epochs + steps for equal training signal\n",
    "\tif args.kshot > 0:\n",
    "\t\t#hard-coded num. of steps of fair kshot evaluation against full model on default numer of epochs\n",
    "\t\tNUM_STEPS = 181500 #num batches in full train data (9075) * 20 epochs \n",
    "\t\tnum_batches = len(train_data)\n",
    "\t\tepochs = NUM_STEPS//num_batches #recalculate number of epochs\n",
    "\t\toverflow_steps = NUM_STEPS%num_batches #num steps in last overflow epoch (if there is one, otherwise 0)\n",
    "\t\tt_total = NUM_STEPS #manually set number of steps for lr schedule\n",
    "\t\tif overflow_steps > 0: epochs+=1 #add extra epoch for overflow steps\n",
    "\t\tprint('Overriding args.epochs and training for {} epochs...'.format(epochs))\n",
    "\n",
    "\t''' \n",
    "\tSET UP FINETUNING MODEL, OPTIMIZER, AND LR SCHEDULE\n",
    "\t'''\n",
    "\tmodel = BiEncoderModel(args.encoder_name, freeze_gloss=args.freeze_gloss, freeze_context=args.freeze_context, \n",
    "                           tie_encoders=args.tie_encoders)\n",
    "\n",
    "\t#speeding up training by putting two encoders on seperate gpus (instead of data parallel)\n",
    "\tif args.multigpu: \n",
    "\t\tmodel.gloss_encoder = model.gloss_encoder.to(gloss_device)\n",
    "\t\tmodel.context_encoder = model.context_encoder.to(context_device)\n",
    "\telse:\n",
    "\t\tmodel = model.cuda()\n",
    "\n",
    "\tcriterion = {}\n",
    "\tif args.balanced:\n",
    "\t\tfor key in train_gloss_dict:\n",
    "\t\t\tcriterion[key] = torch.nn.CrossEntropyLoss(reduction='none', weight=train_gloss_weights[key])\n",
    "\telse:\n",
    "\t\tfor key in train_gloss_dict:\n",
    "\t\t\tcriterion[key] = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "\t#optimize + scheduler from transformers package\n",
    "\t#this taken from transformers finetuning code\n",
    "\tweight_decay = 0.0 #this could be a parameter\n",
    "\tno_decay = ['bias', 'LayerNorm.weight']\n",
    "\toptimizer_grouped_parameters = [\n",
    "\t\t{'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "\t\t{'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "\t\t]\n",
    "\tadam_epsilon = 1e-8\n",
    "\toptimizer = AdamW(optimizer_grouped_parameters, lr=args.lr, eps=adam_epsilon)\n",
    "\t#schedule = WarmupLinearSchedule(optimizer, warmup_steps=args.warmup, t_total=t_total)\n",
    "\tschedule = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup, num_training_steps = t_total)\n",
    "\n",
    "\t'''\n",
    "\tTRAIN MODEL ON SEMCOR DATA\n",
    "\t'''\n",
    "\n",
    "\tbest_dev_f1 = 0.\n",
    "\tprint('Training probe...')\n",
    "\tsys.stdout.flush()\n",
    "\n",
    "\tfor epoch in range(1, epochs+1):\n",
    "\t\t#if last epoch, pass in overflow steps to stop epoch early\n",
    "\t\ttrain_steps = -1\n",
    "\t\tif epoch == epochs and overflow_steps > 0: train_steps = overflow_steps\n",
    "\n",
    "\t\t#train model for one epoch or given number of training steps\n",
    "\t\tmodel, optimizer, schedule, train_loss = _train(train_data, model, train_gloss_dict, optimizer, \n",
    "                                                        schedule, criterion, gloss_bsz=args.gloss_bsz, \n",
    "                                                        max_grad_norm=args.grad_norm, silent=args.silent, \n",
    "                                                        multigpu=args.multigpu, train_steps=train_steps)\n",
    "\n",
    "\t\t#eval model on dev set (semeval2007)\n",
    "\t\teval_preds = _eval(semeval2007_data, model, semeval2007_gloss_dict, multigpu=args.multigpu)\n",
    "\n",
    "\t\t#generate predictions file\n",
    "\t\tpred_filepath = os.path.join(args.ckpt, 'tmp_predictions.txt')\n",
    "\t\twith open(pred_filepath, 'w') as f:\n",
    "\t\t\tfor inst, prediction in eval_preds:\n",
    "\t\t\t\tf.write('{} {}\\n'.format(inst, prediction))\n",
    "\n",
    "\t\t#run predictions through scorer\n",
    "\t\tgold_filepath = os.path.join(args.data_path, 'Evaluation_Datasets/semeval2007/semeval2007.gold.key.txt')\n",
    "\t\tscorer_path = os.path.join(args.data_path, 'Evaluation_Datasets')\n",
    "\t\t_, _, dev_f1 = evaluate_output(scorer_path, gold_filepath, pred_filepath)\n",
    "\t\tprint('Dev f1 after {} epochs = {}'.format(epoch, dev_f1))\n",
    "\t\tsys.stdout.flush() \n",
    "\n",
    "\t\tif dev_f1 >= best_dev_f1:\n",
    "\t\t\tprint('updating best model at epoch {}...'.format(epoch))\n",
    "\t\t\tsys.stdout.flush() \n",
    "\t\t\tbest_dev_f1 = dev_f1\n",
    "\t\t\t#save to file if best probe so far on dev set\n",
    "\t\t\tmodel_fname = os.path.join(args.ckpt, 'best_model.ckpt')\n",
    "\t\t\twith open(model_fname, 'wb') as f:\n",
    "\t\t\t\ttorch.save(model.state_dict(), f)\n",
    "\t\t\tsys.stdout.flush()\n",
    "\n",
    "\t\t#shuffle train set ordering after every epoch\n",
    "\t\trandom.shuffle(train_data)\n",
    "\treturn\n",
    "\n",
    "def evaluate_model(args):\n",
    "\tprint('Evaluating WSD model on {}...'.format(args.split))\n",
    "\n",
    "\t'''\n",
    "\tLOAD TRAINED MODEL\n",
    "\t'''\n",
    "\tmodel = BiEncoderModel(args.encoder_name, freeze_gloss=args.freeze_gloss, freeze_context=args.freeze_context)\n",
    "\tmodel_path = os.path.join(args.ckpt, 'best_model.ckpt')\n",
    "\tmodel.load_state_dict(torch.load(model_path))\n",
    "\tmodel = model.cuda()\n",
    "\t\n",
    "\n",
    "\t'''\n",
    "\tLOAD TOKENIZER\n",
    "\t'''\n",
    "\ttokenizer = load_tokenizer(args.encoder_name)\n",
    "\n",
    "\t'''\n",
    "\tLOAD EVAL SET\n",
    "\t'''\n",
    "\teval_path = os.path.join(args.data_path, 'Evaluation_Datasets/{}/'.format(args.split))\n",
    "\teval_data = load_data(eval_path, args.split)\n",
    "\n",
    "\t#load gloss dictionary (all senses from wordnet for each lemma/pos pair that occur in data)\n",
    "\twn_path = os.path.join(args.data_path, 'Data_Validation/candidatesWN30.txt')\n",
    "\twn_senses = load_wn_senses(wn_path)\n",
    "\tgloss_dict, _ = load_and_preprocess_glosses(eval_data, tokenizer, wn_senses, max_len=32)\n",
    "\n",
    "\teval_data = preprocess_context(tokenizer, eval_data, bsz=1, max_len=-1)\n",
    "\n",
    "\t'''\n",
    "\tEVALUATE MODEL\n",
    "\t'''\n",
    "\teval_preds = _eval(eval_data, model, gloss_dict, multigpu=False)\n",
    "\n",
    "\t#generate predictions file\n",
    "\tpred_filepath = os.path.join(args.ckpt, './{}_predictions.txt'.format(args.split))\n",
    "\twith open(pred_filepath, 'w') as f:\n",
    "\t\tfor inst, prediction in eval_preds:\n",
    "\t\t\tf.write('{} {}\\n'.format(inst, prediction))\n",
    "\n",
    "\t#run predictions through scorer\n",
    "\tgold_filepath = os.path.join(eval_path, '{}.gold.key.txt'.format(args.split))\n",
    "\tscorer_path = os.path.join(args.data_path, 'Evaluation_Datasets')\n",
    "\tp, r, f1 = evaluate_output(scorer_path, gold_filepath, pred_filepath)\n",
    "\tprint('f1 of BERT probe on {} test set = {}'.format(args.split, f1))\n",
    "\n",
    "\treturn\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tif not torch.cuda.is_available():\n",
    "\t\tprint(\"Need available GPU(s) to run this model...\")\n",
    "\t\tquit()\n",
    "\n",
    "\t#parse args\n",
    "\targs = parser.parse_args()\n",
    "\tprint(args)\n",
    "\n",
    "\t#set random seeds\n",
    "\ttorch.manual_seed(args.rand_seed)\n",
    "\tos.environ['PYTHONHASHSEED'] = str(args.rand_seed)\n",
    "\ttorch.cuda.manual_seed(args.rand_seed)\n",
    "\ttorch.cuda.manual_seed_all(args.rand_seed)   \n",
    "\tnp.random.seed(args.rand_seed)\n",
    "\trandom.seed(args.rand_seed)\n",
    "\ttorch.backends.cudnn.benchmark = False\n",
    "\ttorch.backends.cudnn.deterministic=True\n",
    "\n",
    "\t#evaluate model saved at checkpoint or...\n",
    "\tif args.eval: evaluate_model(args)\n",
    "\t#train model\n",
    "\telse: train_model(args)\n",
    "\n",
    "#EOF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:biocreative] *",
   "language": "python",
   "name": "conda-env-biocreative-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
